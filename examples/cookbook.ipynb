{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"display: flex; align-items: center; justify-content: left;\">\n",
    "  <a href=\"https://git.io/typing-svg\"><img src=\"https://readme-typing-svg.demolab.com?font=Georgia&size=32&duration=4000&pause=400&color=FD4578&vCenter=true&multiline=false&width=750&height=100&lines=AI+Function+Cookbook;\" alt=\"Typing SVG\" /></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`weco` is a client facing API for interacting with the [WeCo AI](https://www.weco.ai/) function builder [service](https://weco-app.vercel.app/function). Use this API to build *complex* systems *fast*!\n",
    "\n",
    "Here are a few features our users often ask about. Feel free to follow along:\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/WecoAI/weco-python/blob/main/examples/cookbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<a target=\"_blank\" href=\"https://lightning.ai/new?repo_url=https%3A%2F%2Fgithub.com%2FWecoAI%2Fweco-python%2Fblob%2Fmain%2Fexamples%2Fcookbook.ipynb\"><img src=\"https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg\" alt=\"Open in Studio\" width=100 height=20/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package\n",
    "%pip install weco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export your API key found [here](https://www.aifunction.com/account/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env WECO_API_KEY=<YOUR_WECO_API_KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can build powerful AI functions for complex tasks quickly and without friction. For example, you can create a function in the [web console](https://www.aifunction.com/function/new) with a simple description as shown below:\n",
    "\n",
    "> \"Analyze a business idea and provide a structured evaluation. Output a JSON with 'viability_score' (0-100), 'strengths' (list), 'weaknesses' (list), and 'next_steps' (list).\"\n",
    "\n",
    "Once the function has been built, you can query, test and deploy it anywhere in your code with just a few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weco import query\n",
    "\n",
    "response = query(\n",
    "    fn_name=\"BusinessIdeaAnalyzer-XYZ123\",  # Replace with your actual function name\n",
    "    text_input=\"A subscription service for personalized, AI-generated bedtime stories for children.\"\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our AI functions can interpret complex visual information, follow instructions in natural language and provide practical insights. We accept a variety of different forms of image input:\n",
    "1. Base64 encoding\n",
    "2. Public URL\n",
    "3. Local Path\n",
    "\n",
    "Let's explore how we can have an AI function manage a part of our household. By running this once a month, I am able to find ways to cut down my energy consumption and ultimately save me money!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weco import build, query\n",
    "import base64\n",
    "\n",
    "task_description = \"\"\"\n",
    "You are a smart home energy analyzer that can process images of smart meters, home exteriors, \n",
    "and indoor spaces to provide energy efficiency insights. The analyzer should:\n",
    "    1. Interpret smart meter readings\n",
    "    2. Assess home features relevant to energy consumption\n",
    "    3. Analyze thermostat settings\n",
    "    4. Provide energy-saving recommendations\n",
    "    5. Evaluate renewable energy potential\n",
    "\n",
    "The output should include:\n",
    "    - 'energy_consumption': current usage and comparison to average\n",
    "    - 'home_analysis': visible energy features and potential issues\n",
    "    - 'thermostat_settings': current settings and recommendations\n",
    "    - 'energy_saving_recommendations': actionable suggestions with estimated savings\n",
    "    - 'renewable_energy_potential': assessment of current and potential renewable energy use\n",
    "    - 'estimated_carbon_footprint': current footprint and potential reduction\n",
    "\"\"\"\n",
    "\n",
    "fn_name, _ = build(task_description=task_description)\n",
    "\n",
    "request = \"\"\"\n",
    "Analyze these images of my home and smart meter to provide energy efficiency insights \n",
    "and recommendations for reducing my electricity consumption.\n",
    "\"\"\"\n",
    "\n",
    "# Base64 encoded image\n",
    "with open(\"/path/to/home_exterior.jpeg\", \"rb\") as img_file:\n",
    "    my_home_exterior = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "\n",
    "query_response = query(\n",
    "    fn_name=fn_name,\n",
    "    text_input=request,\n",
    "    images_input=[\n",
    "        \"https://example.com/my_smart_meter_reading.png\",  # Public URL\n",
    "        f\"data:image/jpeg;base64,{my_home_exterior}\",      # Base64 encoding\n",
    "        \"/path/to/living_room_thermostat.jpg\"              # Local image path\n",
    "    ]\n",
    ")\n",
    "\n",
    "for key, value in query_response[\"output\"].items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore what an AI function can do for you and what features we offer through this running example:\n",
    "> \"I want to know if AI can solve a problem for me, how easy it is to arrive at a solution and whether any helpful tips for me along the way. Help me understand this through - 'feasibility', 'justification', and 'suggestions'.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description = \"I want to know if AI can solve a problem for me, how easy it is to arrive at a solution and whether any helpful tips for me along the way. Help me understand this through - 'feasibility', 'justification', and 'suggestions'.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense vs. Sparse Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend building functions in our [web console](https://www.aifunction.com/) for maximum control over the function with the ability to rapidly prototype, test and improve the it. However, you can also do build a function programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weco import build, query\n",
    "\n",
    "# Describe the task you want the function to perform\n",
    "fn_name, fn_desc = build(task_description=task_description)\n",
    "print(f\"AI Function {fn_name} built. This does the following - \\n{fn_desc}.\")\n",
    "\n",
    "# Query the function with a specific input\n",
    "query_response = query(\n",
    "    fn_name=fn_name,\n",
    "    text_input=\"I want to train a model to predict house prices using the Boston Housing dataset hosted on Kaggle.\"\n",
    ")\n",
    "for key, value in query_response[\"output\"].items(): print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend to use the `weco.build` and `weco.query` when you want to build or query LLM functions sparsely, i.e., you **don't** call `weco.build` or `weco.query` in many places within your code. However, for more dense usage, we've found users prefer our `weco.WecoAI` client instance. Its easy to switch between the two as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weco import WecoAI\n",
    "\n",
    "# Connect to our service, using our client\n",
    "client = WecoAI()\n",
    "\n",
    "# Make the same query as before\n",
    "query_response = client.query(\n",
    "    fn_name=fn_name,\n",
    "    text_input=\"I want to train a model to predict house prices using the Boston Housing dataset hosted on Kaggle.\"\n",
    ")\n",
    "for key, value in query_response.items(): print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We understand that sometimes, independent of how many times in your code you call `weco` functions, you want to submit a large batch of requests for the same function you've created. This can be done in the following ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weco import batch_query\n",
    "\n",
    "# Query the same function with multiple inputs by batching them for maximum efficiency\n",
    "input_1 = {\"text_input\": \"I want to train a model to predict house prices using the Boston Housing dataset hosted on Kaggle.\"}\n",
    "input_2 = {\n",
    "    \"text_input\": \"I want to train a model to classify digits using the MNIST dataset hosted on Kaggle using a Google Colab notebook. Attached is an example of what some of the digits would look like.\",\n",
    "    \"images_input\": [\"https://machinelearningmastery.com/wp-content/uploads/2019/02/Plot-of-a-Subset-of-Images-from-the-MNIST-Dataset-1024x768.png\"]\n",
    "}\n",
    "query_responses = batch_query(\n",
    "    fn_names=fn_name,\n",
    "    batch_inputs=[input_1, input_2]\n",
    ")\n",
    "for i, query_response in enumerate(query_responses):\n",
    "    print(\"-\"*50)\n",
    "    print(f\"For input {i + 1}\")\n",
    "    for key, value in query_response[\"output\"].items(): print(f\"{key}: {value}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do the same using the `weco.WecoAI` client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now you've been making synchronous calls to our client by we also support asynchronous programmers. This is actually how we implement batching! You can also make asynchronous calls to our service using our `weco.WecoAI` client or as shown below for the same example as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weco import abuild, aquery\n",
    "\n",
    "# Describe the task you want the function to perform\n",
    "fn_name, fn_desc = await abuild(task_description=task_description)\n",
    "print(f\"AI Function {fn_name} built. This does the following - \\n{fn_desc}.\")\n",
    "\n",
    "# Query the function with a specific input\n",
    "query_response = await aquery(\n",
    "    fn_name=fn_name,\n",
    "    text_input=\"I want to train a model to predict house prices using the Boston Housing dataset hosted on Kaggle.\"\n",
    ")\n",
    "for key, value in query_response[\"output\"].items(): print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now understand why a model generated an output. You'll need to enable Chain of Thought (CoT) for the function version in the [web console](https://www.aifunction.com). You can find this under the **Settings** for a particular function version. Then, to view the model's reasoning behind an output, simply use `return_reasoning=True` at query time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weco import build, query\n",
    "\n",
    "# Describe the task you want the function to perform\n",
    "fn_name, fn_desc = build(task_description=task_description)\n",
    "print(f\"AI Function {fn_name} built. This does the following - \\n{fn_desc}.\")\n",
    "\n",
    "# Query the function with a specific input\n",
    "query_response = query(\n",
    "    fn_name=fn_name,\n",
    "    text_input=\"I want to train a model to predict house prices using the Boston Housing dataset hosted on Kaggle.\",\n",
    "    return_reasoning=True\n",
    ")\n",
    "for key, value in query_response[\"output\"].items(): print(f\"{key}: {value}\")\n",
    "for i, step in enumerate(query_response[\"reasoning_steps\"]): print(f\"Step {i+1}: {step}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
