{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p>A client facing API for interacting with the Weco AI function builder service!</p> <p>Use this API to build complex systems fast. We lower the barrier of entry to software engineer, data science and machine learning by providing an interface to prototype difficult solutions quickly in just a few lines of code.</p>"},{"location":"#what_we_offer","title":"What We Offer","text":"<ul> <li>The build function enables quick and easy prototyping of new functions that use foundational models through just natural language. We encourage users to do this through our web console for maximum control and ease of use, however, you can also do this through our API as shown here.</li> <li>The query function allows you to test your newly created functions and deploy it in your code.</li> </ul> <p>We provide both services in two ways:</p> <ul> <li><code>weco.WecoAI</code> client to be used when you want to maintain the same client service across a portion of code. This is better for dense service usage or in an object oriented paradigm.</li> <li><code>weco.query</code> and <code>weco.build</code> to be used when you only require sparse usage or a functional paradigm.</li> </ul> <p>Some of the key features we provide are: - Structured Output - Grounding (Web Access) - Multimodal (Language &amp; Vision) - Versatile Client (Synchronous, Asynchronous, Batch Processing) - Interpretable (Observe Reasoning Behind Outputs)</p>"},{"location":"#getting_started","title":"Getting Started","text":"<p>Install the <code>weco</code> package simply by calling this in your terminal of choice: <pre><code>pip install weco\n</code></pre></p> <p>When using the Weco API, you will need to set the API key: You can find/setup your API key here. Once you have your API key, pass it directly to the client using the <code>api_key</code> argument or set it as an environment variable as shown: <pre><code>export WECO_API_KEY=&lt;YOUR_WECO_API_KEY&gt;\n</code></pre></p>"},{"location":"#example","title":"Example","text":"<p>We created a function on the web console for the following task:</p> <p>\"Analyze a business idea and provide a structured evaluation. Output a JSON with 'viability_score' (0-100), 'strengths' (list), 'weaknesses' (list), and 'next_steps' (list).\"</p> <p>Here's how you can query this function anywhere in your code! <pre><code>from weco import query\nresponse = query(\n    fn_name=\"BusinessIdeaAnalyzer-XYZ123\",  # Replace with your actual function name\n    text_input=\"A subscription service for personalized, AI-generated bedtime stories for children.\"\n)\n</code></pre> For more examples and an advanced user guide, check out our function builder cookbook.</p> <p>Happy building \\(f\\)(\ud83d\udc77\u200d\u2642\ufe0f)!</p>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing","title":"Contributing","text":"<p>We value your contributions! If you believe you can help to improve our package enabling people to build AI with AI, please contribute!</p> <p>Use the following steps as a guideline to help you make contributions:</p> <ol> <li> <p>Download and install package from source:    <pre><code>git clone https://github.com/WecoAI/weco-python.git\ncd weco-python\npip install -e \".[dev,docs]\"\n</code></pre></p> </li> <li> <p>Create a new branch for your feature or bugfix:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes and run tests to ensure everything is working:</p> </li> </ol> <p>Tests can be expensive to run as they make LLM requests with the API key being used so it is the developers best interests to write small and simple tests that adds coverage for a large portion of the package.</p> <p><pre><code>pytest -n auto tests\n</code></pre>    If you're just making changes to the docs, feel free to skip this step.</p> <ol> <li>Commit and push your changes, then open a PR for us to view \ud83d\ude01</li> </ol> <p>Please ensure your code follows our style guidelines (Numpy docstrings) and includes appropriate tests. We appreciate your contributions!</p>"},{"location":"api/api/","title":"API Reference Guide","text":"<p>The <code>weco</code> package offers two modes of interacting with the AI function service. Each mode is specifically designed for different use cases.</p> Submodule Description Use Cases <code>weco.WecoAI</code> Weco AI client to build and query functions synchronously, asynchronously and in batches. - Dense service usage- Maintaing the same client instance over large portions of code <code>weco.functional</code> Functional form of Weco AI client offering the same features as <code>weco.WecoAI</code>. - Sparse service usage- Quick prototyping"},{"location":"api/client/","title":"WecoAI Client","text":""},{"location":"api/client/#weco.client.WecoAI","title":"WecoAI","text":"<p>A client for the WecoAI function builder API that allows users to build and query specialized functions for GenAI tasks and features. The user must simply provide a task description to build a function, and then query the function with an input to get the result they need. Our client supports both synchronous and asynchronous request paradigms and uses HTTP/2 for faster communication with the API. Support for structured outputs, multimodality, grounding through web search and reasoning is included. For maximum control/flexibility, we recommend that users build functions on the WecoAI platform to leverage the full power of the API, then deploy it in production using this client.</p>"},{"location":"api/client/#weco.client.WecoAI.__init__","title":"__init__","text":"<pre><code>__init__(api_key=None, timeout=120.0, http2=True)\n</code></pre> <p>Initializes the WecoAI client.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key used for authentication. If not provided, the client will attempt to read it from the environment variable - <code>WECO_API_KEY</code>.</p> <code>None</code> <code>timeout</code> <code>float</code> <p>The timeout for the HTTP requests in seconds. Default is 120.</p> <code>120.0</code> <code>http2</code> <code>bool</code> <p>Whether to use HTTP/2 protocol for the HTTP requests. Default is True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API key is not provided to the client, is not set as an environment variable (<code>WECO_API_KEY</code>) or is not a string.</p>"},{"location":"api/client/#weco.client.WecoAI.abuild","title":"abuild  <code>async</code>","text":"<pre><code>abuild(task_description)\n</code></pre> <p>Build a specialized function for a task.</p> <p>Parameters:</p> Name Type Description Default <code>task_description</code> <code>str</code> <p>The description of the task for which the function is being built.</p> required <p>Returns:</p> Type Description <code>tuple[str, int, str]</code> <p>A tuple containing the name, version number and description of the function.</p>"},{"location":"api/client/#weco.client.WecoAI.aquery","title":"aquery  <code>async</code>","text":"<pre><code>aquery(fn_name, version=-1, text_input='', images_input=[], return_reasoning=False, strict=False)\n</code></pre> <p>Queries a specific function with the input (text, images or both) asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>fn_name</code> <code>str</code> <p>The name of the function to query.</p> required <code>version</code> <code>Optional[Union[str, int]]</code> <p>The version alias/number of the function to query. Default is -1 which results in the latest version being used.</p> <code>-1</code> <code>text_input</code> <code>str</code> <p>The text input to the function.</p> <code>''</code> <code>images_input</code> <code>List[str]</code> <p>A list of image URLs or base64 encoded images to be used as input to</p> <code>[]</code> <code>return_reasoning</code> <code>bool</code> <p>A flag to indicate if the reasoning should be returned. Default is False.</p> <code>False</code> <code>strict</code> <code>bool</code> <p>A flag to indicate if the function should be queried in strict mode where the inputs provided should match the input modalities of the LLM chosen for this function. For example, when strict is True, a text-image query to a function that uses a text-only LLM will raise an error. When strict is False, the function will attempt to handle the input by dropping the image components. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the output of the function, the number of input tokens, the number of output tokens, and the latency in milliseconds.</p>"},{"location":"api/client/#weco.client.WecoAI.batch_query","title":"batch_query","text":"<pre><code>batch_query(fn_name, batch_inputs, version=-1, return_reasoning=False, strict=False)\n</code></pre> <p>Performs batch queries on a specified function version.</p> <p>This method processes multiple inputs concurrently using asynchronous queries, ensuring that results are returned in the same order as the inputs.</p> <p>Parameters:</p> Name Type Description Default <code>fn_name</code> <code>str</code> <p>The name of the function to query.</p> required <code>batch_inputs</code> <code>List[Dict[str, Any]]</code> <p>A list of input dictionaries for the function. Each dictionary can include: - \"text_input\": A string for text input. - \"images_input\": A list of image URLs or base64 encoded images.</p> required <code>version</code> <code>Union[str, int]</code> <p>The version alias or number of the function to query. Defaults to -1 for the latest version.</p> <code>-1</code> <code>return_reasoning</code> <code>bool</code> <p>If True, includes reasoning in the response. Defaults to False.</p> <code>False</code> <code>strict</code> <code>bool</code> <p>A flag to indicate if the function should be queried in strict mode where the inputs provided should match the input modalities of the LLM chosen for this function. For example, when strict is True, a text-image query to a function that uses a text-only LLM will raise an error. When strict is False, the function will attempt to handle the input by dropping the image components. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of dictionaries, each containing the result of a function query, including: - The function's output. - The number of input tokens. - The number of output tokens. - The latency in milliseconds.</p>"},{"location":"api/client/#weco.client.WecoAI.build","title":"build","text":"<pre><code>build(task_description)\n</code></pre> <p>Build a specialized function for a task.</p> <p>Parameters:</p> Name Type Description Default <code>task_description</code> <code>str</code> <p>The description of the task for which the function is being built.</p> required <p>Returns:</p> Type Description <code>tuple[str, int, str]</code> <p>A tuple containing the name, version number and description of the function.</p>"},{"location":"api/client/#weco.client.WecoAI.query","title":"query","text":"<pre><code>query(fn_name, version=-1, text_input='', images_input=[], return_reasoning=False, strict=False)\n</code></pre> <p>Queries a specific function with the input (text, images or both).</p> <p>Parameters:</p> Name Type Description Default <code>fn_name</code> <code>str</code> <p>The name of the function to query.</p> required <code>version</code> <code>str | int</code> <p>The version alias/number of the function to query. Default is -1 which results in the latest version being used.</p> <code>-1</code> <code>text_input</code> <code>str</code> <p>The text input to the function.</p> <code>''</code> <code>images_input</code> <code>List[str]</code> <p>A list of image URLs or base64 encoded images to be used as input to the function.</p> <code>[]</code> <code>return_reasoning</code> <code>bool</code> <p>A flag to indicate if the reasoning should be returned. Default is False.</p> <code>False</code> <code>strict</code> <code>bool</code> <p>A flag to indicate if the function should be queried in strict mode where the inputs provided should match the input modalities of the LLM chosen for this function. For example, when strict is True, a text-image query to a function that uses a text-only LLM will raise an error. When strict is False, the function will attempt to handle the input by dropping the image components. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the output of the function, the number of input tokens, the number of output tokens, and the latency in milliseconds.</p>"},{"location":"api/functional/","title":"WecoAI Functions","text":""},{"location":"api/functional/#weco.functional.abuild","title":"abuild  <code>async</code>","text":"<pre><code>abuild(task_description, api_key=None)\n</code></pre> <p>Build a specialized function for a task asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>task_description</code> <code>str</code> <p>The description of the task for which the function is being built.</p> required <code>api_key</code> <code>str</code> <p>The API key for the WecoAI service. If not provided, the API key must be set using the environment variable - <code>WECO_API_KEY</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[str, int, str]</code> <p>A tuple containing the function name, version number and description.</p>"},{"location":"api/functional/#weco.functional.aquery","title":"aquery  <code>async</code>","text":"<pre><code>aquery(fn_name, version=-1, text_input='', images_input=[], return_reasoning=False, strict=False, api_key=None)\n</code></pre> <p>Queries a specific function with the input (text, images or both) asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>fn_name</code> <code>str</code> <p>The name of the function to query.</p> required <code>version</code> <code>Optional[Union[str, int]]</code> <p>The version alias/number of the function to query. Default is -1 which results in the latest version being used.</p> <code>-1</code> <code>text_input</code> <code>str</code> <p>The text input to the function.</p> <code>''</code> <code>images_input</code> <code>List[str]</code> <p>A list of image URLs or base64 encoded images to be used as input to</p> <code>[]</code> <code>return_reasoning</code> <code>bool</code> <p>A flag to indicate if the reasoning should be returned. Default is False.</p> <code>False</code> <code>strict</code> <code>bool</code> <p>A flag to indicate if the function should be queried in strict mode where the inputs provided should match the input modalities of the LLM chosen for this function. For example, when strict is True, a text-image query to a function that uses a text-only LLM will raise an error. When strict is False, the function will attempt to handle the input by dropping the image components. Default is False.</p> <code>False</code> <code>api_key</code> <code>str</code> <p>The API key for the WecoAI service. If not provided, the API key must be set using the environment variable - <code>WECO_API_KEY</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the output of the function, the number of input tokens, the number of output tokens,</p>"},{"location":"api/functional/#weco.functional.batch_query","title":"batch_query","text":"<pre><code>batch_query(fn_name, batch_inputs, version=-1, return_reasoning=False, strict=False, api_key=None)\n</code></pre> <p>Batch queries a single function with multiple inputs.</p> <p>This method uses the asynchronous queries to submit a batch of queries concurrently and waits for all responses to be received before returning the results. Order of the responses corresponds to the order of the inputs.</p> <p>Parameters:</p> Name Type Description Default <code>fn_name</code> <code>str</code> <p>The name of the function to query.</p> required <code>batch_inputs</code> <code>List[Dict[str, Any]]</code> <p>A list of dictionaries, each representing an input for the function. Each dictionary can contain: - \"text_input\": A string for text input. - \"images_input\": A list of image URLs or base64 encoded images.</p> required <code>version</code> <code>str | int</code> <p>The version alias/number of the function to query. Default is -1 which results in the latest version being used.</p> <code>-1</code> <code>return_reasoning</code> <code>bool</code> <p>A flag to indicate if the reasoning should be returned. Default is False.</p> <code>False</code> <code>strict</code> <code>bool</code> <p>A flag to indicate if the function should be queried in strict mode where the inputs provided should match the input modalities of the LLM chosen for this function. For example, when strict is True, a text-image query to a function that uses a text-only LLM will raise an error. When strict is False, the function will attempt to handle the input by dropping the image components. Default is False.</p> <code>False</code> <code>api_key</code> <code>str</code> <p>The API key for the WecoAI service. If not provided, the API key must be set using the environment variable - <code>WECO_API_KEY</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of dictionaries, each containing the result of a function query. Each dictionary includes: - The function's output. - The number of input tokens. - The number of output tokens. - The latency in milliseconds.</p>"},{"location":"api/functional/#weco.functional.build","title":"build","text":"<pre><code>build(task_description, api_key=None)\n</code></pre> <p>Build a specialized function for a task.</p> <p>Parameters:</p> Name Type Description Default <code>task_description</code> <code>str</code> <p>The description of the task for which the function is being built.</p> required <code>api_key</code> <code>str</code> <p>The API key for the WecoAI service. If not provided, the API key must be set using the environment variable - <code>WECO_API_KEY</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[str, int, str]</code> <p>A tuple containing the function name, version number and description.</p>"},{"location":"api/functional/#weco.functional.query","title":"query","text":"<pre><code>query(fn_name, version=-1, text_input='', images_input=[], return_reasoning=False, strict=False, api_key=None)\n</code></pre> <p>Queries a specific function with the input (text, images or both).</p> <p>Parameters:</p> Name Type Description Default <code>fn_name</code> <code>str</code> <p>The name of the function to query.</p> required <code>version</code> <code>str | int</code> <p>The version alias/number of the function to query. Default is -1 which results in the latest version being used.</p> <code>-1</code> <code>text_input</code> <code>str</code> <p>The text input to the function.</p> <code>''</code> <code>images_input</code> <code>List[str]</code> <p>A list of image URLs or base64 encoded images to be used as input to the function.</p> <code>[]</code> <code>return_reasoning</code> <code>bool</code> <p>A flag to indicate if the reasoning should be returned. Default is False.</p> <code>False</code> <code>strict</code> <code>bool</code> <p>A flag to indicate if the function should be queried in strict mode where the inputs provided should match the input modalities of the LLM chosen for this function. For example, when strict is True, a text-image query to a function that uses a text-only LLM will raise an error. When strict is False, the function will attempt to handle the input by dropping the image components. Default is False.</p> <code>False</code> <code>api_key</code> <code>str</code> <p>The API key for the WecoAI service. If not provided, the API key must be set using the environment variable - <code>WECO_API_KEY</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the output of the function, the number of input tokens, the number of output tokens, and the latency in milliseconds.</p>"},{"location":"cookbook/cookbook/","title":"Cookbook","text":""},{"location":"cookbook/cookbook/#getting_started","title":"Getting Started","text":"<p><code>weco</code> is a client facing API for interacting with the Weco AI function builder service. Use this API to build complex systems fast!</p> <p>Here are a few features our users often ask about. Feel free to follow along:</p> <p> </p> <pre><code># Install the package\n%pip install weco\n</code></pre> <p>Export your API key found here.</p> <pre><code>%env WECO_API_KEY=&lt;YOUR_WECO_API_KEY&gt;\n</code></pre> <p>You can build powerful AI functions for complex tasks quickly and without friction. For example, you can create a function in the web console with a simple description as shown below:</p> <p>\"Analyze a business idea and provide a structured evaluation. Output a JSON with 'viability_score' (0-100), 'strengths' (list), 'weaknesses' (list), and 'next_steps' (list).\"</p> <p>Once the function has been built, you can query, test and deploy it anywhere in your code with just a few lines:</p> <pre><code>from weco import query\n\nresponse = query(\n    fn_name=\"BusinessIdeaAnalyzer-XYZ123\",  # Replace with your actual function name\n    text_input=\"A subscription service for personalized, AI-generated bedtime stories for children.\"\n)\n\nprint(response)\n</code></pre>"},{"location":"cookbook/cookbook/#multimodality","title":"Multimodality","text":"<p>Our AI functions can interpret complex visual information, follow instructions in natural language and provide practical insights. We accept a variety of different forms of image input: 1. Base64 encoding 2. Public URL 3. Local Path</p> <p>Let's explore how we can have an AI function manage a part of our household. By running this once a month, I am able to find ways to cut down my energy consumption and ultimately save me money!</p> <pre><code>from weco import build, query\nimport base64\n\ntask_description = \"\"\"\nYou are a smart home energy analyzer that can process images of smart meters, home exteriors, \nand indoor spaces to provide energy efficiency insights. The analyzer should:\n    1. Interpret smart meter readings\n    2. Assess home features relevant to energy consumption\n    3. Analyze thermostat settings\n    4. Provide energy-saving recommendations\n    5. Evaluate renewable energy potential\n\nThe output should include:\n    - 'energy_consumption': current usage and comparison to average\n    - 'home_analysis': visible energy features and potential issues\n    - 'thermostat_settings': current settings and recommendations\n    - 'energy_saving_recommendations': actionable suggestions with estimated savings\n    - 'renewable_energy_potential': assessment of current and potential renewable energy use\n    - 'estimated_carbon_footprint': current footprint and potential reduction\n\"\"\"\n\nfn_name, _ = build(task_description=task_description)\n\nrequest = \"\"\"\nAnalyze these images of my home and smart meter to provide energy efficiency insights \nand recommendations for reducing my electricity consumption.\n\"\"\"\n\n# Base64 encoded image\nwith open(\"/path/to/home_exterior.jpeg\", \"rb\") as img_file:\n    my_home_exterior = base64.b64encode(img_file.read()).decode('utf-8')\n\nquery_response = query(\n    fn_name=fn_name,\n    text_input=request,\n    images_input=[\n        \"https://example.com/my_smart_meter_reading.png\",  # Public URL\n        f\"data:image/jpeg;base64,{my_home_exterior}\",      # Base64 encoding\n        \"/path/to/living_room_thermostat.jpg\"              # Local image path\n    ]\n)\n\nfor key, value in query_response[\"output\"].items():\n    print(f\"{key}: {value}\")\n</code></pre>"},{"location":"cookbook/cookbook/#running_example","title":"Running Example","text":"<p>Let's explore what an AI function can do for you and what features we offer through this running example:</p> <p>\"I want to know if AI can solve a problem for me, how easy it is to arrive at a solution and whether any helpful tips for me along the way. Help me understand this through - 'feasibility', 'justification', and 'suggestions'.\"</p> <pre><code>task_description = \"I want to know if AI can solve a problem for me, how easy it is to arrive at a solution and whether any helpful tips for me along the way. Help me understand this through - 'feasibility', 'justification', and 'suggestions'.\"\n</code></pre>"},{"location":"cookbook/cookbook/#dense_vs_sparse_usage","title":"Dense vs. Sparse Usage","text":"<p>We recommend building functions in our web console for maximum control over the function with the ability to rapidly prototype, test and improve the it. However, you can also do build a function programmatically.</p> <pre><code>from weco import build, query\n\n# Describe the task you want the function to perform\nfn_name, fn_desc = build(task_description=task_description)\nprint(f\"AI Function {fn_name} built. This does the following - \\n{fn_desc}.\")\n\n# Query the function with a specific input\nquery_response = query(\n    fn_name=fn_name,\n    text_input=\"I want to train a model to predict house prices using the Boston Housing dataset hosted on Kaggle.\"\n)\nfor key, value in query_response[\"output\"].items(): print(f\"{key}: {value}\")\n</code></pre> <p>We recommend to use the <code>weco.build</code> and <code>weco.query</code> when you want to build or query LLM functions sparsely, i.e., you don't call <code>weco.build</code> or <code>weco.query</code> in many places within your code. However, for more dense usage, we've found users prefer our <code>weco.WecoAI</code> client instance. Its easy to switch between the two as shown below:</p> <pre><code>from weco import WecoAI\n\n# Connect to our service, using our client\nclient = WecoAI()\n\n# Make the same query as before\nquery_response = client.query(\n    fn_name=fn_name,\n    text_input=\"I want to train a model to predict house prices using the Boston Housing dataset hosted on Kaggle.\"\n)\nfor key, value in query_response.items(): print(f\"{key}: {value}\")\n</code></pre>"},{"location":"cookbook/cookbook/#batching","title":"Batching","text":"<p>We understand that sometimes, independent of how many times in your code you call <code>weco</code> functions, you want to submit a large batch of requests for the same function you've created. This can be done in the following ways:</p> <pre><code>from weco import batch_query\n\n# Query the same function with multiple inputs by batching them for maximum efficiency\ninput_1 = {\"text_input\": \"I want to train a model to predict house prices using the Boston Housing dataset hosted on Kaggle.\"}\ninput_2 = {\n    \"text_input\": \"I want to train a model to classify digits using the MNIST dataset hosted on Kaggle using a Google Colab notebook. Attached is an example of what some of the digits would look like.\",\n    \"images_input\": [\"https://machinelearningmastery.com/wp-content/uploads/2019/02/Plot-of-a-Subset-of-Images-from-the-MNIST-Dataset-1024x768.png\"]\n}\nquery_responses = batch_query(\n    fn_names=fn_name,\n    batch_inputs=[input_1, input_2]\n)\nfor i, query_response in enumerate(query_responses):\n    print(\"-\"*50)\n    print(f\"For input {i + 1}\")\n    for key, value in query_response[\"output\"].items(): print(f\"{key}: {value}\")\n    print(\"-\"*50)\n</code></pre> <p>You can do the same using the <code>weco.WecoAI</code> client.</p>"},{"location":"cookbook/cookbook/#async_calls","title":"Async Calls","text":"<p>Until now you've been making synchronous calls to our client by we also support asynchronous programmers. This is actually how we implement batching! You can also make asynchronous calls to our service using our <code>weco.WecoAI</code> client or as shown below for the same example as before:</p> <pre><code>from weco import abuild, aquery\n\n# Describe the task you want the function to perform\nfn_name, fn_desc = await abuild(task_description=task_description)\nprint(f\"AI Function {fn_name} built. This does the following - \\n{fn_desc}.\")\n\n# Query the function with a specific input\nquery_response = await aquery(\n    fn_name=fn_name,\n    text_input=\"I want to train a model to predict house prices using the Boston Housing dataset hosted on Kaggle.\"\n)\nfor key, value in query_response[\"output\"].items(): print(f\"{key}: {value}\")\n</code></pre>"},{"location":"cookbook/cookbook/#interpretability","title":"Interpretability","text":"<p>You can now understand why a model generated an output. You'll need to enable Chain of Thought (CoT) for the function version in the web console. You can find this under the Settings for a particular function version. Then, to view the model's reasoning behind an output, simply use <code>return_reasoning=True</code> at query time!</p> <pre><code>from weco import build, query\n\n# Describe the task you want the function to perform\nfn_name, fn_desc = build(task_description=task_description)\nprint(f\"AI Function {fn_name} built. This does the following - \\n{fn_desc}.\")\n\n# Query the function with a specific input\nquery_response = query(\n    fn_name=fn_name,\n    text_input=\"I want to train a model to predict house prices using the Boston Housing dataset hosted on Kaggle.\",\n    return_reasoning=True\n)\nfor key, value in query_response[\"output\"].items(): print(f\"{key}: {value}\")\nfor i, step in enumerate(query_response[\"reasoning_steps\"]): print(f\"Step {i+1}: {step}\")\n</code></pre>"},{"location":"getting_started/installation/","title":"Installation","text":"<p>The package is easy to install and keep up to date.</p>"},{"location":"getting_started/installation/#pip","title":"Pip \ud83d\udce6","text":"<p>Install <code>weco</code> using <code>pip</code> - </p> <p><pre><code>pip install weco\n</code></pre> Installing inside a python virtual environment or conda environment is highly recommended.</p> <p>If you already have the package and simply want to upgrade - </p> <pre><code>pip install --upgrade weco\n</code></pre>"},{"location":"getting_started/installation/#source","title":"Source \ud83d\udcd6","text":"<p>You can also install from <code>source</code> - </p> <pre><code>git clone https://github.com/WecoAI/weco-python\ncd weco-python\npython install -e .\n</code></pre> <p>To upgrade the package to the latest version, you can run the following commands - </p> <pre><code>cd weco-python\ngit pull origin main\npython install -e .\n</code></pre>"},{"location":"getting_started/installation/#jump_in","title":"Jump In \ud83c\udfca\u200d\u2642\ufe0f","text":"<p>If you need help getting started, you can check out any of the following resources we provide:</p> <ul> <li>Introduction</li> <li>Cookbook</li> <li>API Reference</li> </ul> <p>Happy building \\(f\\)(\ud83d\udc77\u200d\u2642\ufe0f)!</p>"},{"location":"getting_started/introduction/","title":"Introduction","text":"<p>Lets jump right in!</p>"},{"location":"getting_started/introduction/#export_api_key","title":"Export API Key","text":"<p>When using the Weco API, you will need to set the API key. You can find/setup your API key here. Here's what it looks like.</p> <p></p> <p>Once you have your API key, pass it directly to the client using the <code>api_key</code> argument or set it as an environment variable as below. <pre><code>export WECO_API_KEY=&lt;YOUR_WECO_API_KEY&gt;\n</code></pre></p>"},{"location":"getting_started/introduction/#build_deploy","title":"Build &amp; Deploy","text":"<p>We can create a function on the web console for the following task:</p> <p>\"Analyze a business idea and provide a structured evaluation. Output a JSON with 'viability_score' (0-100), 'strengths' (list), 'weaknesses' (list), and 'next_steps' (list).\"</p> <p>Or just stick to using Python: <pre><code>from weco import build\nfn_name, version_number, fn_description = build(\n    task_description=\"Analyze a business idea and provide a structured evaluation. Output a JSON with 'viability_score' (0-100), 'strengths' (list), 'weaknesses' (list), and 'next_steps' (list).\",\n)\nprint(f\"Function {fn_name}/{version_number} does the following:\\n{fn_description}\")\n</code></pre></p> <p>Here's how you can query this function anywhere in your code! <pre><code>response = query(\n    fn_name=\"BusinessIdeaAnalyzer-XYZ123\",  # Replace with your actual function name\n    text_input=\"A subscription service for personalized, AI-generated bedtime stories for children.\"\n)\n</code></pre> For more examples and an advanced user guide, check out our function builder cookbook and API reference</p> <p>Happy building \\(f\\)(\ud83d\udc77\u200d\u2642\ufe0f)!</p>"}]}